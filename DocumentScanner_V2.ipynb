{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34f57e16-d939-4c7b-9c7e-b6cc8fc8b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Libraries and created local vector db\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from pypdf.errors import FileNotDecryptedError\n",
    "# Globals\n",
    "vectorstore: Optional[FAISS] = None\n",
    "qa_chain: Optional[ConversationalRetrievalChain] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6128e39-1e53-464f-bb3a-c31c61940e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12f50063-adac-41e6-af20-209c2c664eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> Tuple[Optional[List], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Load and process a document from the given file path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the document file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of documents and an error message (if any).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            return None, \"Unsupported file format\"\n",
    "\n",
    "        documents = loader.load()\n",
    "        return documents, None\n",
    "\n",
    "    except FileNotDecryptedError:\n",
    "        return None, \"Error: The PDF file is password-protected and cannot be processed.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23b8bae4-3925-4e14-8ead-71cb84e69e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(documents: List) -> FAISS:\n",
    "    \"\"\"\n",
    "    Create a vectorstore from the provided documents using OpenAI embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62d1970b-9a86-4942-acb9-68693ee53a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_process(file: gr.File) -> str:\n",
    "    \"\"\"\n",
    "    Handle file upload, process the document, and initialize the QA chain.\n",
    "\n",
    "    Parameters:\n",
    "        file (gr.File): The uploaded file to process.\n",
    "\n",
    "    Returns:\n",
    "        str: Status message indicating success or failure.\n",
    "    \"\"\"\n",
    "    global vectorstore, qa_chain\n",
    "\n",
    "    if not file:\n",
    "        return \"Error: No document uploaded. Please upload a file.\"\n",
    "\n",
    "    documents, error = load_document(file.name)\n",
    "    if error:\n",
    "        return error  # Display the error returned by load_document\n",
    "\n",
    "    vectorstore = create_vectorstore(documents)\n",
    "    llm = OpenAI(temperature=0.5)\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
    "\n",
    "    return \"Document processed and ready for chat!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d966522b-53ad-4ea6-86dc-cbc912ef2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_document(query: str, history: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Handle user queries by interacting with the uploaded document.\n",
    "    \"\"\"\n",
    "    if not vectorstore or not qa_chain:\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"Please upload and process a document first.\"})\n",
    "        return history\n",
    "\n",
    "    # Prepare history for the ConversationalRetrievalChain\n",
    "    formatted_history = [(entry[\"role\"], entry[\"content\"]) for entry in history]\n",
    "\n",
    "    inputs = {\"question\": query, \"chat_history\": formatted_history}\n",
    "    result = qa_chain.invoke(inputs)\n",
    "\n",
    "    # Update chat history\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    history.append({\"role\": \"assistant\", \"content\": result['answer']})\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d008e0b-f558-4168-b6c5-b08cb515221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    # Header Section\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 style=\"text-align: center;\">üìö ChatBridge</h1>\n",
    "        <h3 style=\"text-align: center; color: gray;\">Effortlessly Engage with Your Documents</h3>\n",
    "        \"\"\", \n",
    "        elem_id=\"header\"\n",
    "    )\n",
    "\n",
    "    # Instructions Section\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <p><b>How to Use:</b></p>\n",
    "        <ol>\n",
    "            <li>Upload your document by dragging and dropping it below.</li>\n",
    "            <li>Click \"Analyze Document\" to process the uploaded file.</li>\n",
    "            <li>Type your questions in the chat box and click \"Chat\" to interact with the document.</li>\n",
    "        </ol>\n",
    "        <p style=\"color: gray; font-size: small;\">Supported Formats: PDF, Word, Text</p>\n",
    "        \"\"\",\n",
    "        elem_id=\"instructions\"\n",
    "    )\n",
    "\n",
    "    # File Upload and Status\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            file_input = gr.File(label=\"üìÇ Drag & Drop Your File\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
    "            upload_button = gr.Button(\"üìä Analyze Document\", variant=\"primary\")\n",
    "        with gr.Column(scale=2, min_width=400):\n",
    "            status_output = gr.Textbox(\n",
    "                label=\"Status\",\n",
    "                interactive=False,\n",
    "                placeholder=\"Upload a document to start!\",\n",
    "                lines=2,\n",
    "            )\n",
    "\n",
    "    # Chat Section\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(\n",
    "            label=\"üí¨ Ask Anything\",\n",
    "            placeholder=\"Type your question about the document here...\",\n",
    "            lines=2,\n",
    "        )\n",
    "        submit_button = gr.Button(\"üí¨ Chat\", variant=\"primary\")\n",
    "\n",
    "    # Chat History Section\n",
    "    chat_history = gr.Chatbot(label=\"üó®Ô∏è Conversation History\", type=\"messages\", height=400)\n",
    "\n",
    "    # Footer\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <p style=\"text-align: center; color: gray; font-size: small;\">Created by <b>Prithvi</b></p>\n",
    "        \"\"\",\n",
    "        elem_id=\"footer\"\n",
    "    )\n",
    "\n",
    "    # Handle file upload\n",
    "    upload_button.click(upload_and_process, inputs=file_input, outputs=status_output)\n",
    "\n",
    "    # Handle chat functionality\n",
    "    def chat_and_clear(query, chat_history):\n",
    "        updated_chat = chat_with_document(query, chat_history)\n",
    "        return updated_chat, \"\"  # Clear input after each query\n",
    "\n",
    "    submit_button.click(\n",
    "        chat_and_clear,\n",
    "        inputs=[query_input, chat_history],\n",
    "        outputs=[chat_history, query_input],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ee54e39-a0f8-43e8-9919-86d2e36a4611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://16e820c63c57eef95d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://16e820c63c57eef95d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9919ff8a-59fb-4b3a-af32-647b6ded7bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
